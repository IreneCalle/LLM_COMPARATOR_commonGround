{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ù commonGround - Multi-Model Consensus Analyzer\n",
    "\n",
    "A tool to compare responses from multiple LLMs and find common ground through consensus analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "#!pip install gradio openai python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "# Initialize OpenRouter client\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available models (VERIFIED December 2025 - Free + Paid)\n",
    "AVAILABLE_MODELS = [\n",
    "    # Google Gemini - Free, fast, reliable\n",
    "    \"google/gemini-2.0-flash-exp:free\",\n",
    "    \"google/gemini-2.5-flash-lite-preview:free\",\n",
    "    \n",
    "    # Meta Llama - Free, good quality\n",
    "    \"meta-llama/llama-3.3-70b-instruct:free\",\n",
    "    \n",
    "    # Anthropic Claude - PAID but excellent\n",
    "    \"anthropic/claude-3.5-haiku\",\n",
    "    \"anthropic/claude-3.5-sonnet\",\n",
    "    \"anthropic/claude-haiku-4.5\",\n",
    "    \n",
    "    # OpenAI - PAID, very good\n",
    "    \"openai/gpt-4o-mini\",\n",
    "    \n",
    "    # DeepSeek - Free, excellent for reasoning\n",
    "    \"deepseek/deepseek-r1:free\",\n",
    "]\n",
    "\n",
    "EXAMPLE_QUESTIONS = [\n",
    "    \"What are the most important metrics for evaluating LLM performance?\",\n",
    "    \"How do you design an effective prompt for summarization tasks?\",\n",
    "    \"What's the difference between BLEU and ROUGE scores?\",\n",
    "    \"Explain the concept of few-shot learning in LLMs\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(model: str, question: str, temperature: float, top_p: float, max_tokens: int = 500) -> str:\n",
    "    \"\"\"Call a single model and return its response.\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": question}],\n",
    "            temperature=min(temperature, 1.0),\n",
    "            top_p=top_p,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        error_msg = str(e).lower()\n",
    "        if \"429\" in str(e) or \"rate\" in error_msg:\n",
    "            return f\"‚ö†Ô∏è **Rate Limited**: {model} - Free tier exhausted\"\n",
    "        elif \"402\" in str(e) or \"insufficient\" in error_msg:\n",
    "            return f\"üí≥ **Insufficient Credits**: Add credits for {model}\"\n",
    "        elif \"404\" in str(e) or \"not found\" in error_msg:\n",
    "            return f\"‚ùå **Unavailable**: {model} temporarily down\"\n",
    "        elif \"401\" in str(e) or \"auth\" in error_msg:\n",
    "            return f\"‚ùå **Auth Error**: Check OPENROUTER_API_KEY\"\n",
    "        else:\n",
    "            return f\"‚ùå **Error**: {str(e)[:150]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(\n",
    "    question: str,\n",
    "    model1: str, model2: str, model3: str, model4: str,\n",
    "    temperature: float, top_p: float,\n",
    "    progress=gr.Progress()\n",
    ") -> Tuple[str, str, str, str, str, List[str], List[str]]:\n",
    "    \"\"\"Step 1: Generate responses from 4 models.\"\"\"\n",
    "    \n",
    "    # Validate inputs\n",
    "    if not question.strip():\n",
    "        return (\"‚ùå Please enter a question\", \"\", \"\", \"\", \n",
    "                \"‚ö†Ô∏è No question provided\", [], [])\n",
    "    \n",
    "    models = [model1, model2, model3, model4]\n",
    "    if len(set(models)) != 4:\n",
    "        return (\"‚ùå Please select 4 different models\", \"\", \"\", \"\",\n",
    "                \"‚ö†Ô∏è Duplicate models selected\", [], [])\n",
    "    \n",
    "    # Generate responses with progress tracking\n",
    "    responses = []\n",
    "    response_texts = []\n",
    "    \n",
    "    for i, model in enumerate(models, 1):\n",
    "        progress((i-1)/4, desc=f\"ü§ñ Calling {model.split('/')[-1]}...\")\n",
    "        response = call_model(model, question, temperature, top_p)\n",
    "        responses.append(response)\n",
    "        response_texts.append(f\"### ü§ñ Model {i}: {model}\\n\\n{response}\")\n",
    "    \n",
    "    progress(1.0, desc=\"‚úÖ All responses generated!\")\n",
    "    \n",
    "    status_msg = \"‚úÖ **Responses generated successfully!** You can now analyze them.\"\n",
    "    \n",
    "    return (*response_texts, status_msg, responses, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(\n",
    "    question: str,\n",
    "    model1: str, model2: str, model3: str, model4: str,\n",
    "    temperature: float, top_p: float,\n",
    "    progress=gr.Progress()\n",
    ") -> Tuple[str, str, str, str, str, List[str], List[str]]:\n",
    "    \"\"\"Step 1: Generate responses from 4 models.\"\"\"\n",
    "    \n",
    "    # Validate inputs\n",
    "    if not question.strip():\n",
    "        return (\"‚ùå Please enter a question\", \"\", \"\", \"\", \n",
    "                \"‚ö†Ô∏è No question provided\", [], [])\n",
    "    \n",
    "    models = [model1, model2, model3, model4]\n",
    "    if len(set(models)) != 4:\n",
    "        return (\"‚ùå Please select 4 different models\", \"\", \"\", \"\",\n",
    "                \"‚ö†Ô∏è Duplicate models selected\", [], [])\n",
    "    \n",
    "    # Generate responses\n",
    "    responses = []\n",
    "    response_texts = []\n",
    "    \n",
    "    for i, model in enumerate(models, 1):\n",
    "        # Simplified progress - only show percentage\n",
    "        progress((i-1)/4)\n",
    "        response = call_model(model, question, temperature, top_p)\n",
    "        responses.append(response)\n",
    "        response_texts.append(f\"### ü§ñ Model {i}: {model}\\n\\n{response}\")\n",
    "    \n",
    "    status_msg = \"‚úÖ **Responses generated!** Click 'Step 2' to analyze.\"\n",
    "    \n",
    "    return (*response_texts, status_msg, responses, models)\n",
    "\n",
    "\n",
    "def analyze_responses_step(\n",
    "    responses: List[str],\n",
    "    models: List[str],\n",
    "    objective: str,\n",
    "    progress=gr.Progress()\n",
    ") -> Tuple[str, str, str, Dict]:\n",
    "    \"\"\"Step 2: Analyze responses for consensus and metrics.\"\"\"\n",
    "    \n",
    "    if not responses or len(responses) != 4:\n",
    "        return (\"\", \"\", \"‚ö†Ô∏è Please generate responses first!\", {})\n",
    "    \n",
    "    # Check ONLY for actual error messages (starting with emoji)\n",
    "    if any(r.startswith((\"‚ùå\", \"‚ö†Ô∏è\", \"üí≥\")) for r in responses):\n",
    "        return (\"\", \"\", \"‚ö†Ô∏è Some models failed. Please regenerate responses.\", {})\n",
    "    \n",
    "    # Simplified progress\n",
    "    progress(0.3)\n",
    "    \n",
    "    # Create analysis prompt\n",
    "    analysis_prompt = f\"\"\"You are an expert AI evaluator. Analyze these 4 model responses.\n",
    "\n",
    "Objective: {objective}\n",
    "\n",
    "Responses:\n",
    "{''.join([f\"\\n\\nModel {i+1} ({models[i]}):\\n{resp}\" for i, resp in enumerate(responses)])}\n",
    "\n",
    "Provide analysis in JSON format:\n",
    "{{\n",
    "    \"consensus\": [\"point 1\", \"point 2\", \"point 3\"],\n",
    "    \"coherence_score\": \"1-10 with brief explanation\",\n",
    "    \"repetition_simplicity\": \"assessment of redundancy and complexity\",\n",
    "    \"model_metrics\": [\n",
    "        {{\n",
    "            \"model\": \"Model 1\",\n",
    "            \"hallucination\": \"yes/no/NA\",\n",
    "            \"imprecise\": \"yes/no/NA\",\n",
    "            \"off_topic\": \"yes/no/NA\",\n",
    "            \"subjective\": \"yes/no/NA\",\n",
    "            \"overly_enthusiastic\": \"yes/no/NA\",\n",
    "            \"tone\": \"description\"\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Return ONLY valid JSON.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        progress(0.6)\n",
    "        analysis = client.chat.completions.create(\n",
    "            model=\"anthropic/claude-3.5-sonnet\",\n",
    "            messages=[{\"role\": \"user\", \"content\": analysis_prompt}],\n",
    "            temperature=0.3,\n",
    "            max_tokens=1500\n",
    "        )\n",
    "        analysis_data = json.loads(analysis.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        analysis_data = {\n",
    "            \"error\": f\"Analysis failed: {str(e)}\",\n",
    "            \"consensus\": [\"Unable to analyze\"],\n",
    "            \"coherence_score\": \"N/A\",\n",
    "            \"repetition_simplicity\": \"N/A\",\n",
    "            \"model_metrics\": []\n",
    "        }\n",
    "    \n",
    "    # Format consensus\n",
    "    consensus_text = \"### üéØ Consensus Points\\n\\n\"\n",
    "    for i, point in enumerate(analysis_data.get('consensus', []), 1):\n",
    "        consensus_text += f\"{i}. {point}\\n\"\n",
    "    consensus_text += f\"\\n**Coherence Score:** {analysis_data.get('coherence_score', 'N/A')}\\n\"\n",
    "    consensus_text += f\"\\n**Repetition/Simplicity:** {analysis_data.get('repetition_simplicity', 'N/A')}\"\n",
    "    \n",
    "    # Format metrics table\n",
    "    metrics = analysis_data.get('model_metrics', [])\n",
    "    metrics_html = format_metrics_table(metrics)\n",
    "    \n",
    "    status_msg = \"‚úÖ **Analysis complete!** Click 'Step 3' for winner.\"\n",
    "    \n",
    "    return (consensus_text, metrics_html, status_msg, analysis_data)\n",
    "\n",
    "\n",
    "def determine_winner_step(\n",
    "    responses: List[str],\n",
    "    models: List[str],\n",
    "    analysis_data: Dict,\n",
    "    objective: str,\n",
    "    progress=gr.Progress()\n",
    ") -> Tuple[str, str]:\n",
    "    \"\"\"Step 3: Determine the winning model.\"\"\"\n",
    "    \n",
    "    if not responses or not analysis_data:\n",
    "        return (\"\", \"‚ö†Ô∏è Please complete Steps 1 and 2 first!\")\n",
    "    \n",
    "    # Simplified progress\n",
    "    progress(0.5)\n",
    "    \n",
    "    verdict_prompt = f\"\"\"You are an expert judge evaluating LLM responses.\n",
    "\n",
    "Objective: {objective}\n",
    "\n",
    "Responses:\n",
    "{''.join([f\"\\n\\n{models[i]}:\\n{resp}\" for i, resp in enumerate(responses)])}\n",
    "\n",
    "Analysis Summary:\n",
    "- Consensus: {', '.join(analysis_data.get('consensus', []))}\n",
    "- Coherence: {analysis_data.get('coherence_score', 'N/A')}\n",
    "\n",
    "Evaluate each model based on:\n",
    "1. **Objective fulfillment**: Did it meet the stated objective?\n",
    "2. **Clarity**: Clear and well-structured?\n",
    "3. **Consensus alignment**: Aligns with common ground?\n",
    "4. **Structure**: Well-organized?\n",
    "\n",
    "Declare the WINNER and provide brief justification (3-4 sentences).\"\"\"\n",
    "    \n",
    "    try:\n",
    "        verdict = client.chat.completions.create(\n",
    "            model=\"anthropic/claude-3.5-sonnet\",\n",
    "            messages=[{\"role\": \"user\", \"content\": verdict_prompt}],\n",
    "            temperature=0.5,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        verdict_text = f\"### üèÜ Final Verdict\\n\\n{verdict.choices[0].message.content.strip()}\"\n",
    "    except Exception as e:\n",
    "        verdict_text = f\"### üèÜ Final Verdict\\n\\nUnable to determine verdict: {str(e)}\"\n",
    "    \n",
    "    status_msg = \"‚úÖ **All done!** Winner determined.\"\n",
    "    \n",
    "    return (verdict_text, status_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_winner_step(\n",
    "    responses: List[str],\n",
    "    models: List[str],\n",
    "    analysis_data: Dict,\n",
    "    objective: str,\n",
    "    progress=gr.Progress()\n",
    ") -> Tuple[str, str]:\n",
    "    \"\"\"Step 3: Determine the winning model.\"\"\"\n",
    "    \n",
    "    if not responses or not analysis_data:\n",
    "        return (\"\", \"‚ö†Ô∏è Please generate and analyze responses first!\")\n",
    "    \n",
    "    progress(0.5, desc=\"üèÜ Evaluating models to determine winner...\")\n",
    "    \n",
    "    verdict_prompt = f\"\"\"You are an expert judge evaluating LLM responses.\n",
    "\n",
    "Objective: {objective}\n",
    "\n",
    "Responses:\n",
    "{''.join([f\"\\n\\n{models[i]}:\\n{resp}\" for i, resp in enumerate(responses)])}\n",
    "\n",
    "Analysis Summary:\n",
    "- Consensus: {', '.join(analysis_data.get('consensus', []))}\n",
    "- Coherence: {analysis_data.get('coherence_score', 'N/A')}\n",
    "\n",
    "Evaluate each model based on:\n",
    "1. **Objective fulfillment**: Did it meet the stated objective?\n",
    "2. **Clarity**: Clear and well-structured?\n",
    "3. **Consensus alignment**: Aligns with common ground?\n",
    "4. **Structure**: Well-organized?\n",
    "\n",
    "Declare the WINNER and provide brief justification (3-4 sentences).\"\"\"\n",
    "    \n",
    "    try:\n",
    "        verdict = client.chat.completions.create(\n",
    "            model=\"anthropic/claude-3.5-sonnet\",\n",
    "            messages=[{\"role\": \"user\", \"content\": verdict_prompt}],\n",
    "            temperature=0.5,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        verdict_text = f\"### üèÜ Final Verdict\\n\\n{verdict.choices[0].message.content.strip()}\"\n",
    "    except Exception as e:\n",
    "        verdict_text = f\"### üèÜ Final Verdict\\n\\nUnable to determine verdict: {str(e)}\"\n",
    "    \n",
    "    progress(1.0, desc=\"‚úÖ Winner determined!\")\n",
    "    \n",
    "    status_msg = \"‚úÖ **Winner determined!** Analysis complete.\"\n",
    "    \n",
    "    return (verdict_text, status_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_metrics_table(metrics: List[Dict]) -> str:\n",
    "    \"\"\"Format model metrics as HTML table.\"\"\"\n",
    "    if not metrics:\n",
    "        return \"<p>No metrics available</p>\"\n",
    "    \n",
    "    html = \"\"\"<table style='width:100%; border-collapse: collapse; margin-top: 20px;'>\n",
    "    <tr style='background-color: #f0f0f0;'>\n",
    "        <th style='border: 1px solid #ddd; padding: 8px;'>Model</th>\n",
    "        <th style='border: 1px solid #ddd; padding: 8px;'>Hallucination</th>\n",
    "        <th style='border: 1px solid #ddd; padding: 8px;'>Imprecise</th>\n",
    "        <th style='border: 1px solid #ddd; padding: 8px;'>Off-Topic</th>\n",
    "        <th style='border: 1px solid #ddd; padding: 8px;'>Subjective</th>\n",
    "        <th style='border: 1px solid #ddd; padding: 8px;'>Overly Enthusiastic</th>\n",
    "        <th style='border: 1px solid #ddd; padding: 8px;'>Tone</th>\n",
    "    </tr>\"\"\"\n",
    "    \n",
    "    for metric in metrics:\n",
    "        html += f\"\"\"<tr>\n",
    "        <td style='border: 1px solid #ddd; padding: 8px;'><strong>{metric.get('model', 'N/A')}</strong></td>\n",
    "        <td style='border: 1px solid #ddd; padding: 8px; text-align: center;'>{metric.get('hallucination', 'NA')}</td>\n",
    "        <td style='border: 1px solid #ddd; padding: 8px; text-align: center;'>{metric.get('imprecise', 'NA')}</td>\n",
    "        <td style='border: 1px solid #ddd; padding: 8px; text-align: center;'>{metric.get('off_topic', 'NA')}</td>\n",
    "        <td style='border: 1px solid #ddd; padding: 8px; text-align: center;'>{metric.get('subjective', 'NA')}</td>\n",
    "        <td style='border: 1px solid #ddd; padding: 8px; text-align: center;'>{metric.get('overly_enthusiastic', 'NA')}</td>\n",
    "        <td style='border: 1px solid #ddd; padding: 8px;'>{metric.get('tone', 'N/A')}</td>\n",
    "    </tr>\"\"\"\n",
    "    \n",
    "    html += \"</table>\"\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* Running on public URL: https://64ad9fa86867d92381.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://64ad9fa86867d92381.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Gradio interface with 3-step workflow\n",
    "with gr.Blocks(title=\"commonGround - Multi-Model Consensus\", theme=gr.themes.Soft()) as demo:\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    # ü§ù commonGround\n",
    "    ### Multi-Model Consensus Analyzer\n",
    "    \n",
    "    Compare responses from 4 different LLMs through a **3-step workflow**:\n",
    "    1. üöÄ Generate responses from selected models\n",
    "    2. üìä Analyze consensus and quality metrics  \n",
    "    3. üèÜ Determine the winning model\n",
    "    \"\"\")\n",
    "    \n",
    "    # === INPUTS SECTION ===\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            question_input = gr.Textbox(\n",
    "                label=\"‚ùì Your Question\",\n",
    "                placeholder=\"Enter your question here...\",\n",
    "                lines=3,\n",
    "                value=EXAMPLE_QUESTIONS[0]\n",
    "            )\n",
    "            \n",
    "            objective_input = gr.Textbox(\n",
    "                label=\"üéØ Objective (What should the ideal response achieve?)\",\n",
    "                placeholder=\"E.g., Provide a comprehensive explanation with examples...\",\n",
    "                lines=2,\n",
    "                value=\"Provide an accurate, clear, and comprehensive response with practical examples\"\n",
    "            )\n",
    "            \n",
    "            gr.Markdown(\"### ü§ñ Select 4 Different Models\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                model1 = gr.Dropdown(\n",
    "                    choices=AVAILABLE_MODELS,\n",
    "                    value=AVAILABLE_MODELS[0],\n",
    "                    label=\"Model 1\"\n",
    "                )\n",
    "                model2 = gr.Dropdown(\n",
    "                    choices=AVAILABLE_MODELS,\n",
    "                    value=AVAILABLE_MODELS[2],\n",
    "                    label=\"Model 2\"\n",
    "                )\n",
    "            \n",
    "            with gr.Row():\n",
    "                model3 = gr.Dropdown(\n",
    "                    choices=AVAILABLE_MODELS,\n",
    "                    value=AVAILABLE_MODELS[3],\n",
    "                    label=\"Model 3\"\n",
    "                )\n",
    "                model4 = gr.Dropdown(\n",
    "                    choices=AVAILABLE_MODELS,\n",
    "                    value=AVAILABLE_MODELS[6],\n",
    "                    label=\"Model 4\"\n",
    "                )\n",
    "        \n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"### ‚öôÔ∏è Parameters\")\n",
    "            \n",
    "            temperature = gr.Slider(\n",
    "                minimum=0.0,\n",
    "                maximum=2.0,\n",
    "                value=0.7,\n",
    "                step=0.1,\n",
    "                label=\"Temperature\",\n",
    "                info=\"Higher = more creative\"\n",
    "            )\n",
    "            \n",
    "            top_p = gr.Slider(\n",
    "                minimum=0.0,\n",
    "                maximum=1.0,\n",
    "                value=0.9,\n",
    "                step=0.05,\n",
    "                label=\"Top P\",\n",
    "                info=\"Nucleus sampling\"\n",
    "            )\n",
    "            \n",
    "            gr.Markdown(\"### üìö Examples\")\n",
    "            gr.Examples(\n",
    "                examples=[[q] for q in EXAMPLE_QUESTIONS],\n",
    "                inputs=[question_input],\n",
    "                label=\"Try these:\"\n",
    "            )\n",
    "    \n",
    "    # === 3 ACTION BUTTONS ===\n",
    "    gr.Markdown(\"---\")\n",
    "    gr.Markdown(\"### üéØ Workflow Steps\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        btn_generate = gr.Button(\n",
    "            \"üöÄ Step 1: Generate Responses\", \n",
    "            variant=\"primary\", \n",
    "            size=\"lg\",\n",
    "            scale=1\n",
    "        )\n",
    "        btn_analyze = gr.Button(\n",
    "            \"üìä Step 2: Analyze & Evaluate\", \n",
    "            variant=\"secondary\", \n",
    "            size=\"lg\",\n",
    "            scale=1\n",
    "        )\n",
    "        btn_verdict = gr.Button(\n",
    "            \"üèÜ Step 3: Determine Winner\", \n",
    "            variant=\"secondary\", \n",
    "            size=\"lg\",\n",
    "            scale=1\n",
    "        )\n",
    "    \n",
    "    # Status message\n",
    "    status_msg = gr.Markdown(\n",
    "        \"üí° **Start by clicking 'Generate Responses' to begin the analysis**\",\n",
    "        elem_classes=\"status-message\"\n",
    "    )\n",
    "    \n",
    "    # === HIDDEN STATE VARIABLES ===\n",
    "    responses_state = gr.State([])  # Store responses\n",
    "    models_state = gr.State([])  # Store model names\n",
    "    analysis_state = gr.State({})  # Store analysis data\n",
    "    \n",
    "    # === OUTPUTS SECTION ===\n",
    "    gr.Markdown(\"---\")\n",
    "    gr.Markdown(\"## üìä Model Responses\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        response1 = gr.Markdown()\n",
    "        response2 = gr.Markdown()\n",
    "    \n",
    "    with gr.Row():\n",
    "        response3 = gr.Markdown()\n",
    "        response4 = gr.Markdown()\n",
    "    \n",
    "    gr.Markdown(\"---\")\n",
    "    gr.Markdown(\"## üîç Analysis Results\")\n",
    "    \n",
    "    consensus_output = gr.Markdown()\n",
    "    \n",
    "    gr.Markdown(\"### üìã Quality Metrics Table\")\n",
    "    metrics_output = gr.HTML()\n",
    "    \n",
    "    verdict_output = gr.Markdown()\n",
    "    \n",
    "    # === BUTTON CONNECTIONS ===\n",
    "    \n",
    "    # Step 1: Generate Responses\n",
    "    btn_generate.click(\n",
    "        fn=generate_responses,\n",
    "        inputs=[\n",
    "            question_input, model1, model2, model3, model4,\n",
    "            temperature, top_p\n",
    "        ],\n",
    "        outputs=[\n",
    "            response1, response2, response3, response4,\n",
    "            status_msg, responses_state, models_state\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Step 2: Analyze\n",
    "    btn_analyze.click(\n",
    "        fn=analyze_responses_step,\n",
    "        inputs=[responses_state, models_state, objective_input],\n",
    "        outputs=[consensus_output, metrics_output, status_msg, analysis_state]\n",
    "    )\n",
    "    \n",
    "    # Step 3: Determine Winner\n",
    "    btn_verdict.click(\n",
    "        fn=determine_winner_step,\n",
    "        inputs=[responses_state, models_state, analysis_state, objective_input],\n",
    "        outputs=[verdict_output, status_msg]\n",
    "    )\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    ---\n",
    "    **Note:** This tool uses OpenRouter to access multiple LLM providers. Make sure your API key is set in the `.env` file.\n",
    "    \n",
    "    **Workflow Tips:**\n",
    "    - You can review responses before analyzing (saves tokens!)\n",
    "    - Each step builds on the previous one\n",
    "    - Watch the progress indicators for real-time feedback\n",
    "    \"\"\")\n",
    "\n",
    "# Launch the app\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True, server_name=\"0.0.0.0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
