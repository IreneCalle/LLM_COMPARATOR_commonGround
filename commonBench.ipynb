{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä commonBench v2 - Professional LLM Benchmarking Tool\n",
    "\n",
    "## Objetivo\n",
    "Herramienta profesional para evaluar LLMs con metricas cuantitativas completas:\n",
    "\n",
    "### M√©tricas Implementadas\n",
    "\n",
    "| Tier | M√©tricas | Descripci√≥n |\n",
    "|------|----------|-------------|\n",
    "| **Tier 1 - Performance** | `latency_sec`, `ttft_sec`, `tokens_per_sec` | Velocidad y rendimiento |\n",
    "| **Tier 2 - Cost** | `cost_usd`, `cost_per_word`, `projected_monthly` | Costes reales y proyectados |\n",
    "| **Tier 3 - Quality** | `lexical_diversity`, `readability_flesch` | Calidad del texto |\n",
    "| **Tier 4 - Operational** | `error_rate`, `availability`, `retry_count` | Fiabilidad del servicio |\n",
    "| **Tier 5 - Optional** | `f1_score` | Con respuesta de referencia |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar paquetes necesarios\n",
    "#!pip install gradio openai python-dotenv textstat sentence-transformers scikit-learn pandas matplotlib seaborn requests tenacity --quiet\n",
    "\n",
    "#print(\"‚úÖ Dependencias instaladas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìã PASO 2: Imports y configuraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textstat'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Third party - Metrics\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtextstat\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'textstat'"
     ]
    }
   ],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timedelta\n",
    "from enum import Enum\n",
    "from functools import wraps\n",
    "from pathlib import Path\n",
    "\n",
    "# Third party - API\n",
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "\n",
    "# Third party - Metrics\n",
    "import textstat\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Third party - Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s | %(levelname)s | %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment\n",
    "load_dotenv(override=True)\n",
    "API_KEY = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "if not API_KEY:\n",
    "    print(\"‚ö†Ô∏è OPENROUTER_API_KEY no encontrada en .env\")\n",
    "    print(\"   Crea un archivo .env con: OPENROUTER_API_KEY=tu-api-key\")\n",
    "else:\n",
    "    print(f\"‚úÖ API Key cargada: {API_KEY[:8]}...{API_KEY[-4:]}\")\n",
    "\n",
    "print(\"‚úÖ Imports cargados correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìã PASO 3: Sistema de clasificaci√≥n de errores\n",
    "Distinguimos entre errores transitorios (retry) y permanentes (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorType(Enum):\n",
    "    \"\"\"\n",
    "    Clasificaci√≥n de errores para evaluaci√≥n de calidad operativa.\n",
    "    \n",
    "    - TRANSIENT: Retry puede funcionar (timeouts, errores de red)\n",
    "    - PERMANENT: No reintentar (modelo no existe, API key inv√°lida)\n",
    "    - RATE_LIMIT: L√≠mites de la API (esperar y reintentar)\n",
    "    - MODEL_ERROR: Modelo rechaz√≥ la petici√≥n (content policy)\n",
    "    - SUCCESS: No es error\n",
    "    \"\"\"\n",
    "    TRANSIENT = \"transient\"\n",
    "    PERMANENT = \"permanent\"\n",
    "    RATE_LIMIT = \"rate_limit\"\n",
    "    MODEL_ERROR = \"model_error\"\n",
    "    SUCCESS = \"success\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelCallResult:\n",
    "    \"\"\"\n",
    "    Resultado estructurado de una llamada al modelo.\n",
    "    Incluye m√©tricas y metadata para an√°lisis completo.\n",
    "    \"\"\"\n",
    "    success: bool\n",
    "    text: str = \"\"\n",
    "    input_tokens: int = 0\n",
    "    output_tokens: int = 0\n",
    "    total_tokens: int = 0\n",
    "    latency_sec: float = 0.0\n",
    "    ttft_sec: Optional[float] = None  # Time to First Token\n",
    "    error_type: ErrorType = ErrorType.SUCCESS\n",
    "    error_message: str = \"\"\n",
    "    retry_count: int = 0\n",
    "    model_id: str = \"\"\n",
    "\n",
    "\n",
    "def classify_error(exception: Exception) -> Tuple[ErrorType, str]:\n",
    "    \"\"\"\n",
    "    Clasifica una excepci√≥n para decidir si reintentar.\n",
    "    \"\"\"\n",
    "    error_str = str(exception).lower()\n",
    "    error_class = type(exception).__name__\n",
    "    \n",
    "    # Rate limits\n",
    "    if any(kw in error_str for kw in ['rate limit', 'rate_limit', '429', 'too many requests']):\n",
    "        return ErrorType.RATE_LIMIT, f\"Rate limit: {str(exception)[:100]}\"\n",
    "    \n",
    "    # Timeouts\n",
    "    if any(kw in error_str for kw in ['timeout', 'timed out', 'deadline']):\n",
    "        return ErrorType.TRANSIENT, f\"Timeout: {str(exception)[:100]}\"\n",
    "    \n",
    "    # Conexi√≥n\n",
    "    if any(kw in error_str for kw in ['connection', 'network', 'socket', 'ssl']):\n",
    "        return ErrorType.TRANSIENT, f\"Conexi√≥n: {str(exception)[:100]}\"\n",
    "    \n",
    "    # Server errors (5xx)\n",
    "    if any(kw in error_str for kw in ['500', '502', '503', '504', 'server error']):\n",
    "        return ErrorType.TRANSIENT, f\"Server error: {str(exception)[:100]}\"\n",
    "    \n",
    "    # Modelo no encontrado\n",
    "    if any(kw in error_str for kw in ['not found', '404', 'model not available', 'invalid model']):\n",
    "        return ErrorType.PERMANENT, f\"Modelo no disponible: {str(exception)[:100]}\"\n",
    "    \n",
    "    # Auth errors\n",
    "    if any(kw in error_str for kw in ['unauthorized', '401', 'invalid api key', 'authentication']):\n",
    "        return ErrorType.PERMANENT, f\"Auth error: {str(exception)[:100]}\"\n",
    "    \n",
    "    # Content policy\n",
    "    if any(kw in error_str for kw in ['content policy', 'safety', 'refused']):\n",
    "        return ErrorType.MODEL_ERROR, f\"Rechazado: {str(exception)[:100]}\"\n",
    "    \n",
    "    # Default: asumir transitorio\n",
    "    return ErrorType.TRANSIENT, f\"Error ({error_class}): {str(exception)[:100]}\"\n",
    "\n",
    "\n",
    "print(\"‚úÖ Sistema de clasificaci√≥n de errores definido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìã PASO 4: Retry con exponential backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry_with_backoff(\n",
    "    max_retries: int = 3,\n",
    "    base_delay: float = 1.0,\n",
    "    max_delay: float = 30.0,\n",
    "    exponential_base: float = 2.0,\n",
    "    retryable_errors: Tuple[ErrorType, ...] = (ErrorType.TRANSIENT, ErrorType.RATE_LIMIT)\n",
    "):\n",
    "    \"\"\"\n",
    "    Decorador para retry con exponential backoff.\n",
    "    \n",
    "    - Espera m√°s tiempo entre cada reintento\n",
    "    - A√±ade jitter para evitar thundering herd\n",
    "    - Solo reintenta errores transitorios\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs) -> ModelCallResult:\n",
    "            last_result = None\n",
    "            \n",
    "            for attempt in range(max_retries + 1):\n",
    "                result = func(*args, **kwargs)\n",
    "                result.retry_count = attempt\n",
    "                \n",
    "                # √âxito o error permanente: retornar\n",
    "                if result.success or result.error_type not in retryable_errors:\n",
    "                    return result\n",
    "                \n",
    "                last_result = result\n",
    "                \n",
    "                # Calcular delay con backoff + jitter\n",
    "                if attempt < max_retries:\n",
    "                    delay = min(base_delay * (exponential_base ** attempt), max_delay)\n",
    "                    jitter = delay * 0.2 * (random.random() - 0.5)\n",
    "                    actual_delay = delay + jitter\n",
    "                    \n",
    "                    logger.warning(\n",
    "                        f\"Retry {attempt + 1}/{max_retries} - \"\n",
    "                        f\"{result.error_type.value}: esperando {actual_delay:.1f}s\"\n",
    "                    )\n",
    "                    time.sleep(actual_delay)\n",
    "            \n",
    "            return last_result\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "\n",
    "print(\"‚úÖ Sistema de retry definido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìã PASO 5: Colector de estad√≠sticas de errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelErrorStats:\n",
    "    \"\"\"\n",
    "    Estad√≠sticas de errores por modelo.\n",
    "    El error_rate es una m√©trica de calidad operativa.\n",
    "    \"\"\"\n",
    "    total_calls: int = 0\n",
    "    successful_calls: int = 0\n",
    "    failed_calls: int = 0\n",
    "    transient_errors: int = 0\n",
    "    permanent_errors: int = 0\n",
    "    rate_limit_errors: int = 0\n",
    "    model_errors: int = 0\n",
    "    total_retries: int = 0\n",
    "    \n",
    "    @property\n",
    "    def error_rate(self) -> float:\n",
    "        if self.total_calls == 0:\n",
    "            return 0.0\n",
    "        return round(self.failed_calls / self.total_calls * 100, 2)\n",
    "    \n",
    "    @property\n",
    "    def availability(self) -> float:\n",
    "        return round(100 - self.error_rate, 2)\n",
    "\n",
    "\n",
    "class ErrorStatsCollector:\n",
    "    \"\"\"Colector de estad√≠sticas de errores para m√∫ltiples modelos.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._stats: Dict[str, ModelErrorStats] = {}\n",
    "    \n",
    "    def record(self, result: ModelCallResult) -> None:\n",
    "        model_id = result.model_id\n",
    "        \n",
    "        if model_id not in self._stats:\n",
    "            self._stats[model_id] = ModelErrorStats()\n",
    "        \n",
    "        stats = self._stats[model_id]\n",
    "        stats.total_calls += 1\n",
    "        stats.total_retries += result.retry_count\n",
    "        \n",
    "        if result.success:\n",
    "            stats.successful_calls += 1\n",
    "        else:\n",
    "            stats.failed_calls += 1\n",
    "            if result.error_type == ErrorType.TRANSIENT:\n",
    "                stats.transient_errors += 1\n",
    "            elif result.error_type == ErrorType.PERMANENT:\n",
    "                stats.permanent_errors += 1\n",
    "            elif result.error_type == ErrorType.RATE_LIMIT:\n",
    "                stats.rate_limit_errors += 1\n",
    "            elif result.error_type == ErrorType.MODEL_ERROR:\n",
    "                stats.model_errors += 1\n",
    "    \n",
    "    def get_stats(self, model_id: str) -> Optional[ModelErrorStats]:\n",
    "        return self._stats.get(model_id)\n",
    "    \n",
    "    def to_dataframe(self) -> pd.DataFrame:\n",
    "        rows = []\n",
    "        for model_id, stats in self._stats.items():\n",
    "            rows.append({\n",
    "                \"model\": model_id,\n",
    "                \"total_calls\": stats.total_calls,\n",
    "                \"successful\": stats.successful_calls,\n",
    "                \"failed\": stats.failed_calls,\n",
    "                \"error_rate_%\": stats.error_rate,\n",
    "                \"availability_%\": stats.availability,\n",
    "                \"total_retries\": stats.total_retries,\n",
    "            })\n",
    "        return pd.DataFrame(rows)\n",
    "    \n",
    "    def reset(self):\n",
    "        self._stats = {}\n",
    "\n",
    "\n",
    "print(\"‚úÖ Colector de estad√≠sticas de errores definido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìã PASO 6: Sistema de precios din√°micos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelPricing:\n",
    "    \"\"\"\n",
    "    Estructura de precios con metadata para auditor√≠a.\n",
    "    \"\"\"\n",
    "    model_id: str\n",
    "    name: str\n",
    "    input_price_per_million: float\n",
    "    output_price_per_million: float\n",
    "    context_length: int = 0\n",
    "    is_free: bool = False\n",
    "    fetched_at: datetime = field(default_factory=datetime.now)\n",
    "    source: str = \"unknown\"  # \"api\", \"cache\", \"fallback\"\n",
    "    \n",
    "    def calculate_cost(self, input_tokens: int, output_tokens: int) -> float:\n",
    "        input_cost = (input_tokens * self.input_price_per_million) / 1_000_000\n",
    "        output_cost = (output_tokens * self.output_price_per_million) / 1_000_000\n",
    "        return round(input_cost + output_cost, 6)\n",
    "\n",
    "\n",
    "# Precios fallback verificados (Diciembre 2024)\n",
    "FALLBACK_PRICES = {\n",
    "    # Anthropic\n",
    "    \"anthropic/claude-3.5-haiku\": {\"input\": 0.80, \"output\": 4.0, \"name\": \"Claude 3.5 Haiku\"},\n",
    "    \"anthropic/claude-3.5-haiku-20241022\": {\"input\": 0.80, \"output\": 4.0, \"name\": \"Claude 3.5 Haiku\"},\n",
    "    \"anthropic/claude-3.5-sonnet\": {\"input\": 3.0, \"output\": 15.0, \"name\": \"Claude 3.5 Sonnet\"},\n",
    "    \"anthropic/claude-sonnet-4\": {\"input\": 3.0, \"output\": 15.0, \"name\": \"Claude Sonnet 4\"},\n",
    "    \"anthropic/claude-haiku-4.5\": {\"input\": 1.0, \"output\": 5.0, \"name\": \"Claude Haiku 4.5\"},\n",
    "    \n",
    "    # OpenAI\n",
    "    \"openai/gpt-4o-mini\": {\"input\": 0.15, \"output\": 0.60, \"name\": \"GPT-4o Mini\"},\n",
    "    \"openai/gpt-4o\": {\"input\": 2.50, \"output\": 10.0, \"name\": \"GPT-4o\"},\n",
    "    \n",
    "    # Google Free\n",
    "    \"google/gemini-2.0-flash-exp:free\": {\"input\": 0, \"output\": 0, \"name\": \"Gemini 2.0 Flash (Free)\"},\n",
    "    \"google/gemma-3-27b-it:free\": {\"input\": 0, \"output\": 0, \"name\": \"Gemma 3 27B (Free)\"},\n",
    "    \n",
    "    # Meta Free\n",
    "    \"meta-llama/llama-3.3-70b-instruct:free\": {\"input\": 0, \"output\": 0, \"name\": \"Llama 3.3 70B (Free)\"},\n",
    "    \"meta-llama/llama-4-maverick:free\": {\"input\": 0, \"output\": 0, \"name\": \"Llama 4 Maverick (Free)\"},\n",
    "    \n",
    "    # DeepSeek\n",
    "    \"deepseek/deepseek-r1:free\": {\"input\": 0, \"output\": 0, \"name\": \"DeepSeek R1 (Free)\"},\n",
    "    \"deepseek/deepseek-chat-v3-0324:free\": {\"input\": 0, \"output\": 0, \"name\": \"DeepSeek V3 (Free)\"},\n",
    "    \"deepseek/deepseek-chat\": {\"input\": 0.14, \"output\": 0.28, \"name\": \"DeepSeek Chat\"},\n",
    "    \n",
    "    # Qwen\n",
    "    \"qwen/qwq-32b:free\": {\"input\": 0, \"output\": 0, \"name\": \"Qwen QwQ 32B (Free)\"},\n",
    "}\n",
    "\n",
    "\n",
    "class DynamicPricingClient:\n",
    "    \"\"\"\n",
    "    Cliente para obtener precios din√°micos de OpenRouter.\n",
    "    \n",
    "    Caracter√≠sticas:\n",
    "    - Cach√© con TTL configurable\n",
    "    - Fallback a precios conocidos\n",
    "    - Timestamp para auditor√≠a\n",
    "    \"\"\"\n",
    "    \n",
    "    OPENROUTER_MODELS_URL = \"https://openrouter.ai/api/v1/models\"\n",
    "    \n",
    "    def __init__(self, api_key: Optional[str] = None, cache_ttl_hours: float = 1.0):\n",
    "        self.api_key = api_key\n",
    "        self.cache_ttl = timedelta(hours=cache_ttl_hours)\n",
    "        self._cache: Dict[str, ModelPricing] = {}\n",
    "        self._last_fetch: Optional[datetime] = None\n",
    "        self._all_models: List[Dict] = []\n",
    "    \n",
    "    def _is_cache_valid(self) -> bool:\n",
    "        if not self._last_fetch:\n",
    "            return False\n",
    "        return datetime.now() - self._last_fetch < self.cache_ttl\n",
    "    \n",
    "    def _fetch_models_from_api(self) -> List[Dict]:\n",
    "        headers = {}\n",
    "        if self.api_key:\n",
    "            headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(\n",
    "                self.OPENROUTER_MODELS_URL,\n",
    "                headers=headers,\n",
    "                timeout=30\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            models = data.get(\"data\", [])\n",
    "            logger.info(f\"Obtenidos {len(models)} modelos desde OpenRouter API\")\n",
    "            return models\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error obteniendo modelos de API: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _parse_model_pricing(self, model_data: Dict) -> ModelPricing:\n",
    "        model_id = model_data.get(\"id\", \"\")\n",
    "        name = model_data.get(\"name\", model_id)\n",
    "        pricing = model_data.get(\"pricing\", {})\n",
    "        \n",
    "        try:\n",
    "            input_per_token = float(pricing.get(\"prompt\", \"0\") or \"0\")\n",
    "            output_per_token = float(pricing.get(\"completion\", \"0\") or \"0\")\n",
    "        except (ValueError, TypeError):\n",
    "            input_per_token = 0\n",
    "            output_per_token = 0\n",
    "        \n",
    "        input_per_million = input_per_token * 1_000_000\n",
    "        output_per_million = output_per_token * 1_000_000\n",
    "        \n",
    "        return ModelPricing(\n",
    "            model_id=model_id,\n",
    "            name=name,\n",
    "            input_price_per_million=round(input_per_million, 4),\n",
    "            output_price_per_million=round(output_per_million, 4),\n",
    "            context_length=model_data.get(\"context_length\", 0),\n",
    "            is_free=(input_per_million == 0 and output_per_million == 0),\n",
    "            fetched_at=datetime.now(),\n",
    "            source=\"api\"\n",
    "        )\n",
    "    \n",
    "    def refresh_prices(self, force: bool = False) -> bool:\n",
    "        if not force and self._is_cache_valid():\n",
    "            return False\n",
    "        \n",
    "        models = self._fetch_models_from_api()\n",
    "        if not models:\n",
    "            return False\n",
    "        \n",
    "        self._all_models = models\n",
    "        self._last_fetch = datetime.now()\n",
    "        \n",
    "        for model_data in models:\n",
    "            pricing = self._parse_model_pricing(model_data)\n",
    "            self._cache[pricing.model_id] = pricing\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def get_model_pricing(self, model_id: str) -> ModelPricing:\n",
    "        # Refrescar si cach√© expirado\n",
    "        if not self._is_cache_valid():\n",
    "            self.refresh_prices()\n",
    "        \n",
    "        # Buscar en cach√©\n",
    "        if model_id in self._cache:\n",
    "            return self._cache[model_id]\n",
    "        \n",
    "        # Buscar variantes\n",
    "        base_id = model_id.split(\":\")[0]\n",
    "        for cached_id, pricing in self._cache.items():\n",
    "            if cached_id.split(\":\")[0] == base_id:\n",
    "                return pricing\n",
    "        \n",
    "        # Fallback\n",
    "        if model_id in FALLBACK_PRICES:\n",
    "            fb = FALLBACK_PRICES[model_id]\n",
    "            return ModelPricing(\n",
    "                model_id=model_id,\n",
    "                name=fb.get(\"name\", model_id),\n",
    "                input_price_per_million=fb[\"input\"],\n",
    "                output_price_per_million=fb[\"output\"],\n",
    "                is_free=(fb[\"input\"] == 0 and fb[\"output\"] == 0),\n",
    "                source=\"fallback\"\n",
    "            )\n",
    "        \n",
    "        # Desconocido\n",
    "        return ModelPricing(\n",
    "            model_id=model_id,\n",
    "            name=model_id,\n",
    "            input_price_per_million=0,\n",
    "            output_price_per_million=0,\n",
    "            is_free=True,\n",
    "            source=\"unknown\"\n",
    "        )\n",
    "    \n",
    "    def list_available_models(self, free_only: bool = False) -> List[ModelPricing]:\n",
    "        if not self._cache:\n",
    "            self.refresh_prices()\n",
    "        \n",
    "        models = list(self._cache.values())\n",
    "        if free_only:\n",
    "            models = [m for m in models if m.is_free]\n",
    "        \n",
    "        models.sort(key=lambda m: m.input_price_per_million + m.output_price_per_million)\n",
    "        return models\n",
    "    \n",
    "    def get_pricing_summary(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"fetched_at\": self._last_fetch.isoformat() if self._last_fetch else None,\n",
    "            \"total_models\": len(self._cache),\n",
    "            \"free_models\": len([m for m in self._cache.values() if m.is_free]),\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"‚úÖ Sistema de precios din√°micos definido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìã PASO 7: Funci√≥n principal de llamada al modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_caller(client: OpenAI):\n",
    "    \"\"\"\n",
    "    Factory que crea la funci√≥n de llamada al modelo con el cliente configurado.\n",
    "    \"\"\"\n",
    "    \n",
    "    @retry_with_backoff(max_retries=3, base_delay=1.0)\n",
    "    def call_model_with_metrics(\n",
    "        model: str,\n",
    "        question: str,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.9,\n",
    "        max_tokens: int = 500,\n",
    "        stream: bool = True\n",
    "    ) -> ModelCallResult:\n",
    "        \"\"\"\n",
    "        Llama a un modelo con m√©tricas completas incluyendo TTFT.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        ttft = None\n",
    "        \n",
    "        try:\n",
    "            if stream:\n",
    "                # Modo streaming para capturar TTFT\n",
    "                response_chunks = []\n",
    "                first_token_received = False\n",
    "                \n",
    "                stream_response = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": question}],\n",
    "                    temperature=min(temperature, 1.0),\n",
    "                    top_p=top_p,\n",
    "                    max_tokens=max_tokens,\n",
    "                    stream=True\n",
    "                )\n",
    "                \n",
    "                for chunk in stream_response:\n",
    "                    if not first_token_received:\n",
    "                        if chunk.choices and chunk.choices[0].delta.content:\n",
    "                            ttft = time.time() - start_time\n",
    "                            first_token_received = True\n",
    "                    \n",
    "                    if chunk.choices and chunk.choices[0].delta.content:\n",
    "                        response_chunks.append(chunk.choices[0].delta.content)\n",
    "                \n",
    "                text = \"\".join(response_chunks)\n",
    "                latency = time.time() - start_time\n",
    "                \n",
    "                # Estimar tokens (streaming no siempre da usage)\n",
    "                input_tokens = len(question) // 4 + 10\n",
    "                output_tokens = len(text) // 4\n",
    "            \n",
    "            else:\n",
    "                # Modo normal\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": question}],\n",
    "                    temperature=min(temperature, 1.0),\n",
    "                    top_p=top_p,\n",
    "                    max_tokens=max_tokens\n",
    "                )\n",
    "                \n",
    "                latency = time.time() - start_time\n",
    "                text = response.choices[0].message.content.strip()\n",
    "                \n",
    "                usage = response.usage\n",
    "                input_tokens = usage.prompt_tokens if usage else len(question) // 4\n",
    "                output_tokens = usage.completion_tokens if usage else len(text) // 4\n",
    "            \n",
    "            return ModelCallResult(\n",
    "                success=True,\n",
    "                text=text,\n",
    "                input_tokens=input_tokens,\n",
    "                output_tokens=output_tokens,\n",
    "                total_tokens=input_tokens + output_tokens,\n",
    "                latency_sec=round(latency, 3),\n",
    "                ttft_sec=round(ttft, 3) if ttft else None,\n",
    "                error_type=ErrorType.SUCCESS,\n",
    "                model_id=model\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            latency = time.time() - start_time\n",
    "            error_type, error_message = classify_error(e)\n",
    "            \n",
    "            return ModelCallResult(\n",
    "                success=False,\n",
    "                latency_sec=round(latency, 3),\n",
    "                error_type=error_type,\n",
    "                error_message=error_message,\n",
    "                model_id=model\n",
    "            )\n",
    "    \n",
    "    return call_model_with_metrics\n",
    "\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de llamada al modelo definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìã PASO 8: Funciones de m√©tricas de calidad de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lexical_diversity(text: str) -> float:\n",
    "    \"\"\"Type-Token Ratio (unique words / total words).\"\"\"\n",
    "    words = text.lower().split()\n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "    return round(len(set(words)) / len(words), 3)\n",
    "\n",
    "\n",
    "def calculate_readability(text: str) -> float:\n",
    "    \"\"\"Flesch Reading Ease (0-100, higher = easier).\"\"\"\n",
    "    try:\n",
    "        return round(textstat.flesch_reading_ease(text), 1)\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def count_words(text: str) -> int:\n",
    "    \"\"\"Cuenta palabras en el texto.\"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "\n",
    "def count_sentences(text: str) -> int:\n",
    "    \"\"\"Cuenta oraciones en el texto.\"\"\"\n",
    "    try:\n",
    "        return textstat.sentence_count(text)\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def calculate_f1_score(response: str, reference: str) -> float:\n",
    "    \"\"\"F1 score a nivel de palabras.\"\"\"\n",
    "    if not reference or not response:\n",
    "        return 0\n",
    "    \n",
    "    response_words = set(response.lower().split())\n",
    "    reference_words = set(reference.lower().split())\n",
    "    \n",
    "    if len(response_words) == 0 or len(reference_words) == 0:\n",
    "        return 0\n",
    "    \n",
    "    true_positives = len(response_words & reference_words)\n",
    "    precision = true_positives / len(response_words)\n",
    "    recall = true_positives / len(reference_words)\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    \n",
    "    return round(2 * precision * recall / (precision + recall), 3)\n",
    "\n",
    "\n",
    "def calculate_tokens_per_second(tokens: int, latency: float) -> float:\n",
    "    \"\"\"Tokens generados por segundo.\"\"\"\n",
    "    if latency <= 0:\n",
    "        return 0\n",
    "    return round(tokens / latency, 1)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Funciones de m√©tricas definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìã PASO 9: Clase principal del Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonBenchV2:\n",
    "    \"\"\"\n",
    "    Benchmark profesional de LLMs.\n",
    "    \n",
    "    Caracter√≠sticas:\n",
    "    - Precios din√°micos desde API\n",
    "    - TTFT (Time to First Token)\n",
    "    - Error rate como m√©trica\n",
    "    - Retry autom√°tico\n",
    "    - Metadata completa\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: Optional[str] = None):\n",
    "        self.api_key = api_key or os.getenv('OPENROUTER_API_KEY')\n",
    "        \n",
    "        # Cliente OpenRouter\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            api_key=self.api_key\n",
    "        )\n",
    "        \n",
    "        # Funci√≥n de llamada con retry\n",
    "        self.call_model = create_model_caller(self.client)\n",
    "        \n",
    "        # Cliente de precios\n",
    "        self.pricing_client = DynamicPricingClient(api_key=self.api_key)\n",
    "        \n",
    "        # Colector de errores\n",
    "        self.error_collector = ErrorStatsCollector()\n",
    "        \n",
    "        # Estado\n",
    "        self.last_results: List[Dict] = []\n",
    "    \n",
    "    def estimate_cost(\n",
    "        self,\n",
    "        models: List[str],\n",
    "        iterations: int,\n",
    "        avg_input_tokens: int = 100,\n",
    "        avg_output_tokens: int = 400\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Estima el coste ANTES de ejecutar el benchmark.\n",
    "        \"\"\"\n",
    "        total_cost = 0.0\n",
    "        model_costs = {}\n",
    "        \n",
    "        for model_id in models:\n",
    "            pricing = self.pricing_client.get_model_pricing(model_id)\n",
    "            cost_per_call = pricing.calculate_cost(avg_input_tokens, avg_output_tokens)\n",
    "            model_total = cost_per_call * iterations\n",
    "            \n",
    "            model_costs[model_id] = {\n",
    "                \"name\": pricing.name,\n",
    "                \"cost_per_call\": cost_per_call,\n",
    "                \"total_cost\": model_total,\n",
    "                \"is_free\": pricing.is_free,\n",
    "                \"source\": pricing.source\n",
    "            }\n",
    "            total_cost += model_total\n",
    "        \n",
    "        return {\n",
    "            \"total_estimated_cost_usd\": round(total_cost, 4),\n",
    "            \"total_calls\": len(models) * iterations,\n",
    "            \"models\": model_costs\n",
    "        }\n",
    "    \n",
    "    def run_benchmark(\n",
    "        self,\n",
    "        question: str,\n",
    "        models: List[str],\n",
    "        iterations: int = 5,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.9,\n",
    "        max_tokens: int = 500,\n",
    "        reference_text: Optional[str] = None,\n",
    "        stream_for_ttft: bool = True,\n",
    "        progress_callback=None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Ejecuta el benchmark completo.\n",
    "        \"\"\"\n",
    "        # Resetear estado\n",
    "        self.last_results = []\n",
    "        self.error_collector.reset()\n",
    "        \n",
    "        # Refrescar precios\n",
    "        self.pricing_client.refresh_prices()\n",
    "        \n",
    "        total_calls = len(models) * iterations\n",
    "        current_call = 0\n",
    "        \n",
    "        for iteration in range(iterations):\n",
    "            for model_id in models:\n",
    "                current_call += 1\n",
    "                \n",
    "                # Progreso\n",
    "                if progress_callback:\n",
    "                    progress_callback(\n",
    "                        current_call / total_calls,\n",
    "                        f\"üìä Iter {iteration + 1}/{iterations} - {model_id.split('/')[-1]}\"\n",
    "                    )\n",
    "                \n",
    "                # Llamar al modelo\n",
    "                result = self.call_model(\n",
    "                    model=model_id,\n",
    "                    question=question,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    max_tokens=max_tokens,\n",
    "                    stream=stream_for_ttft\n",
    "                )\n",
    "                \n",
    "                # Registrar errores\n",
    "                self.error_collector.record(result)\n",
    "                \n",
    "                # Obtener pricing\n",
    "                pricing = self.pricing_client.get_model_pricing(model_id)\n",
    "                cost = pricing.calculate_cost(result.input_tokens, result.output_tokens)\n",
    "                \n",
    "                # Construir registro\n",
    "                record = self._build_record(\n",
    "                    iteration=iteration + 1,\n",
    "                    result=result,\n",
    "                    pricing=pricing,\n",
    "                    cost=cost,\n",
    "                    question=question,\n",
    "                    reference_text=reference_text\n",
    "                )\n",
    "                \n",
    "                self.last_results.append(record)\n",
    "        \n",
    "        return pd.DataFrame(self.last_results)\n",
    "    \n",
    "    def _build_record(\n",
    "        self,\n",
    "        iteration: int,\n",
    "        result: ModelCallResult,\n",
    "        pricing: ModelPricing,\n",
    "        cost: float,\n",
    "        question: str,\n",
    "        reference_text: Optional[str]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Construye un registro completo con todas las m√©tricas.\n",
    "        \"\"\"\n",
    "        record = {\n",
    "            # Identificaci√≥n\n",
    "            \"iteration\": iteration,\n",
    "            \"model_id\": result.model_id,\n",
    "            \"model_name\": pricing.name,\n",
    "            \n",
    "            # Estado\n",
    "            \"success\": result.success,\n",
    "            \"error_type\": result.error_type.value if not result.success else None,\n",
    "            \"retry_count\": result.retry_count,\n",
    "            \n",
    "            # Tokens\n",
    "            \"input_tokens\": result.input_tokens,\n",
    "            \"output_tokens\": result.output_tokens,\n",
    "            \"total_tokens\": result.total_tokens,\n",
    "            \n",
    "            # Latencia\n",
    "            \"latency_sec\": result.latency_sec,\n",
    "            \"ttft_sec\": result.ttft_sec,\n",
    "            \"tokens_per_sec\": calculate_tokens_per_second(\n",
    "                result.output_tokens, result.latency_sec\n",
    "            ),\n",
    "            \n",
    "            # Coste\n",
    "            \"cost_usd\": cost,\n",
    "            \"is_free\": pricing.is_free,\n",
    "            \"pricing_source\": pricing.source,\n",
    "        }\n",
    "        \n",
    "        # M√©tricas de calidad (solo si exitoso)\n",
    "        if result.success and result.text:\n",
    "            text = result.text\n",
    "            record.update({\n",
    "                \"word_count\": count_words(text),\n",
    "                \"sentence_count\": count_sentences(text),\n",
    "                \"lexical_diversity\": calculate_lexical_diversity(text),\n",
    "                \"readability_flesch\": calculate_readability(text),\n",
    "                \"response_preview\": text[:150] + \"...\" if len(text) > 150 else text,\n",
    "            })\n",
    "            \n",
    "            # F1 si hay referencia\n",
    "            if reference_text:\n",
    "                record[\"f1_score\"] = calculate_f1_score(text, reference_text)\n",
    "        \n",
    "        return record\n",
    "    \n",
    "    def get_aggregated_results(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Agrega resultados por modelo.\n",
    "        \"\"\"\n",
    "        if not self.last_results:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        df = pd.DataFrame(self.last_results)\n",
    "        df_success = df[df['success'] == True].copy()\n",
    "        \n",
    "        if df_success.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # M√©tricas a agregar\n",
    "        numeric_cols = [\n",
    "            'latency_sec', 'ttft_sec', 'tokens_per_sec',\n",
    "            'input_tokens', 'output_tokens', 'cost_usd',\n",
    "            'word_count', 'lexical_diversity', 'readability_flesch'\n",
    "        ]\n",
    "        \n",
    "        # Filtrar columnas existentes\n",
    "        numeric_cols = [c for c in numeric_cols if c in df_success.columns]\n",
    "        \n",
    "        # Agregar\n",
    "        agg_df = df_success.groupby('model_name')[numeric_cols].agg(\n",
    "            ['mean', 'std', 'min', 'max']\n",
    "        ).round(3)\n",
    "        \n",
    "        agg_df.columns = ['_'.join(col) for col in agg_df.columns]\n",
    "        agg_df = agg_df.reset_index()\n",
    "        \n",
    "        return agg_df\n",
    "    \n",
    "    def get_error_stats(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Estad√≠sticas de errores por modelo.\n",
    "        \"\"\"\n",
    "        return self.error_collector.to_dataframe()\n",
    "\n",
    "\n",
    "print(\"‚úÖ Clase CommonBenchV2 definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìã PASO 10: Funciones de visualizaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_chart(df: pd.DataFrame) -> Optional[plt.Figure]:\n",
    "    \"\"\"\n",
    "    Crea gr√°fico de comparaci√≥n: Cost vs Latency.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return None\n",
    "    \n",
    "    # Filtrar solo exitosos\n",
    "    df_success = df[df['success'] == True].copy()\n",
    "    if df_success.empty:\n",
    "        return None\n",
    "    \n",
    "    # Agregar por modelo\n",
    "    model_stats = df_success.groupby('model_name').agg({\n",
    "        'cost_usd': 'mean',\n",
    "        'latency_sec': 'mean',\n",
    "        'tokens_per_sec': 'mean',\n",
    "        'readability_flesch': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Crear figura\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Scatter plot\n",
    "    scatter = ax.scatter(\n",
    "        model_stats['latency_sec'],\n",
    "        model_stats['cost_usd'] * 1000,  # Mostrar en mil√©simas\n",
    "        s=model_stats['tokens_per_sec'] * 3,  # Tama√±o = velocidad\n",
    "        c=model_stats['readability_flesch'],\n",
    "        cmap='RdYlGn',\n",
    "        alpha=0.7,\n",
    "        edgecolors='black'\n",
    "    )\n",
    "    \n",
    "    # Labels\n",
    "    for idx, row in model_stats.iterrows():\n",
    "        ax.annotate(\n",
    "            row['model_name'],\n",
    "            (row['latency_sec'], row['cost_usd'] * 1000),\n",
    "            xytext=(5, 5),\n",
    "            textcoords='offset points',\n",
    "            fontsize=9,\n",
    "            fontweight='bold'\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel('Latencia (segundos)', fontsize=12)\n",
    "    ax.set_ylabel('Coste (mil√©simas de USD)', fontsize=12)\n",
    "    ax.set_title(\n",
    "        'Comparaci√≥n de Modelos: Coste vs Velocidad\\n'\n",
    "        '(Tama√±o = tokens/sec, Color = legibilidad)',\n",
    "        fontsize=14\n",
    "    )\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label('Readability Score', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_ttft_chart(df: pd.DataFrame) -> Optional[plt.Figure]:\n",
    "    \"\"\"\n",
    "    Crea gr√°fico de TTFT por modelo.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return None\n",
    "    \n",
    "    df_success = df[(df['success'] == True) & (df['ttft_sec'].notna())].copy()\n",
    "    if df_success.empty:\n",
    "        return None\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    # Box plot de TTFT\n",
    "    models = df_success['model_name'].unique()\n",
    "    data = [df_success[df_success['model_name'] == m]['ttft_sec'].values for m in models]\n",
    "    \n",
    "    bp = ax.boxplot(data, labels=models, patch_artist=True)\n",
    "    \n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(models)))\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "    \n",
    "    ax.set_ylabel('Time to First Token (segundos)', fontsize=12)\n",
    "    ax.set_title('TTFT por Modelo (M√©trica de UX)', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "print(\"‚úÖ Funciones de visualizaci√≥n definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìã PASO 11: Interfaz Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar benchmark global\n",
    "benchmark = CommonBenchV2()\n",
    "\n",
    "# Obtener modelos disponibles\n",
    "print(\"Cargando modelos disponibles...\")\n",
    "benchmark.pricing_client.refresh_prices()\n",
    "\n",
    "# Modelos recomendados para UI\n",
    "RECOMMENDED_MODELS = [\n",
    "    # Free tier\n",
    "    \"google/gemini-2.0-flash-exp:free\",\n",
    "    \"meta-llama/llama-3.3-70b-instruct:free\",\n",
    "    \"deepseek/deepseek-r1:free\",\n",
    "    \"deepseek/deepseek-chat-v3-0324:free\",\n",
    "    \"qwen/qwq-32b:free\",\n",
    "    # Budget tier\n",
    "    \"openai/gpt-4o-mini\",\n",
    "    \"anthropic/claude-3.5-haiku\",\n",
    "    \"deepseek/deepseek-chat\",\n",
    "    # Mid tier\n",
    "    \"anthropic/claude-3.5-sonnet\",\n",
    "    \"openai/gpt-4o\",\n",
    "]\n",
    "\n",
    "EXAMPLE_QUESTIONS = [\n",
    "    \"Explain quantum entanglement in simple terms\",\n",
    "    \"What are the key differences between supervised and unsupervised learning?\",\n",
    "    \"How does photosynthesis work?\",\n",
    "    \"Explain the concept of blockchain technology\",\n",
    "    \"What is recursion in programming? Give a simple example.\",\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ {len(RECOMMENDED_MODELS)} modelos recomendados disponibles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark_ui(\n",
    "    question: str,\n",
    "    model1: str, model2: str, model3: str, model4: str,\n",
    "    iterations: int,\n",
    "    temperature: float,\n",
    "    top_p: float,\n",
    "    max_tokens: int,\n",
    "    reference_file,\n",
    "    progress=gr.Progress()\n",
    "):\n",
    "    \"\"\"\n",
    "    Wrapper para la UI de Gradio.\n",
    "    \"\"\"\n",
    "    # Validar\n",
    "    if not question.strip():\n",
    "        return None, None, None, None, None, \"‚ùå Por favor ingresa una pregunta\"\n",
    "    \n",
    "    # Obtener modelos seleccionados\n",
    "    models = [m for m in [model1, model2, model3, model4] if m]\n",
    "    if not models:\n",
    "        return None, None, None, None, None, \"‚ùå Selecciona al menos un modelo\"\n",
    "    \n",
    "    # Leer referencia si existe\n",
    "    reference_text = None\n",
    "    if reference_file is not None:\n",
    "        try:\n",
    "            if isinstance(reference_file, bytes):\n",
    "                reference_text = reference_file.decode('utf-8')\n",
    "            else:\n",
    "                with open(reference_file.name, 'r') as f:\n",
    "                    reference_text = f.read()\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error leyendo referencia: {e}\")\n",
    "    \n",
    "    # Mostrar estimaci√≥n de coste\n",
    "    estimate = benchmark.estimate_cost(models, iterations)\n",
    "    \n",
    "    # Ejecutar benchmark\n",
    "    df = benchmark.run_benchmark(\n",
    "        question=question,\n",
    "        models=models,\n",
    "        iterations=iterations,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens,\n",
    "        reference_text=reference_text,\n",
    "        stream_for_ttft=True,\n",
    "        progress_callback=lambda p, m: progress(p, desc=m)\n",
    "    )\n",
    "    \n",
    "    if df.empty:\n",
    "        return None, None, None, None, None, \"‚ùå Todas las llamadas fallaron\"\n",
    "    \n",
    "    # Resultados agregados\n",
    "    agg_df = benchmark.get_aggregated_results()\n",
    "    \n",
    "    # Estad√≠sticas de errores\n",
    "    error_df = benchmark.get_error_stats()\n",
    "    \n",
    "    # Gr√°ficos\n",
    "    chart1 = create_comparison_chart(df)\n",
    "    chart2 = create_ttft_chart(df)\n",
    "    \n",
    "    # Status\n",
    "    successful = len(df[df['success'] == True])\n",
    "    total = len(df)\n",
    "    cost_actual = df[df['success'] == True]['cost_usd'].sum()\n",
    "    \n",
    "    status = (\n",
    "        f\"‚úÖ Benchmark completado\\n\"\n",
    "        f\"üìä {successful}/{total} llamadas exitosas\\n\"\n",
    "        f\"üí∞ Coste real: ${cost_actual:.4f} (estimado: ${estimate['total_estimated_cost_usd']:.4f})\"\n",
    "    )\n",
    "    \n",
    "    return df, agg_df, error_df, chart1, chart2, status\n",
    "\n",
    "\n",
    "def export_to_csv(df):\n",
    "    \"\"\"Exporta resultados a CSV.\"\"\"\n",
    "    if df is None or (isinstance(df, pd.DataFrame) and df.empty):\n",
    "        return None\n",
    "    \n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    csv_path = f\"/tmp/commonbench_results_{timestamp}.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    return csv_path\n",
    "\n",
    "\n",
    "def get_cost_estimate(model1, model2, model3, model4, iterations):\n",
    "    \"\"\"Obtiene estimaci√≥n de coste para mostrar en UI.\"\"\"\n",
    "    models = [m for m in [model1, model2, model3, model4] if m]\n",
    "    if not models:\n",
    "        return \"Selecciona al menos un modelo\"\n",
    "    \n",
    "    estimate = benchmark.estimate_cost(models, iterations)\n",
    "    \n",
    "    lines = [f\"**Estimaci√≥n de coste:**\"]\n",
    "    lines.append(f\"- Total: **${estimate['total_estimated_cost_usd']:.4f}**\")\n",
    "    lines.append(f\"- Llamadas: {estimate['total_calls']}\")\n",
    "    lines.append(\"\")\n",
    "    \n",
    "    for model_id, info in estimate['models'].items():\n",
    "        if info['is_free']:\n",
    "            lines.append(f\"- {info['name']}: üÜì GRATIS\")\n",
    "        else:\n",
    "            lines.append(f\"- {info['name']}: ${info['total_cost']:.4f}\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Funciones de UI definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear interfaz Gradio\n",
    "with gr.Blocks(\n",
    "    title=\"commonBench v2 - Professional LLM Benchmarking\",\n",
    "    theme=gr.themes.Soft()\n",
    ") as demo:\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    # üìä commonBench v2\n",
    "    ### Professional LLM Benchmarking Tool\n",
    "    \n",
    "    Eval√∫a m√∫ltiples LLMs con m√©tricas completas:\n",
    "    - ‚è±Ô∏è **Latencia y TTFT** (Time to First Token)\n",
    "    - üí∞ **Costes reales** (precios din√°micos desde API)\n",
    "    - üìù **Calidad del texto** (legibilidad, diversidad l√©xica)\n",
    "    - ‚ö†Ô∏è **Fiabilidad** (error rate, availability)\n",
    "    \"\"\")\n",
    "    \n",
    "    # === INPUTS ===\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            question_input = gr.Textbox(\n",
    "                label=\"‚ùì Pregunta a evaluar\",\n",
    "                placeholder=\"Escribe tu pregunta aqu√≠...\",\n",
    "                lines=3,\n",
    "                value=EXAMPLE_QUESTIONS[0]\n",
    "            )\n",
    "            \n",
    "            gr.Markdown(\"### ü§ñ Selecciona Modelos (1-4)\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                model1 = gr.Dropdown(\n",
    "                    choices=RECOMMENDED_MODELS,\n",
    "                    value=RECOMMENDED_MODELS[0],\n",
    "                    label=\"Modelo 1\"\n",
    "                )\n",
    "                model2 = gr.Dropdown(\n",
    "                    choices=RECOMMENDED_MODELS,\n",
    "                    value=RECOMMENDED_MODELS[5],\n",
    "                    label=\"Modelo 2\"\n",
    "                )\n",
    "            \n",
    "            with gr.Row():\n",
    "                model3 = gr.Dropdown(\n",
    "                    choices=[None] + RECOMMENDED_MODELS,\n",
    "                    value=None,\n",
    "                    label=\"Modelo 3 (opcional)\"\n",
    "                )\n",
    "                model4 = gr.Dropdown(\n",
    "                    choices=[None] + RECOMMENDED_MODELS,\n",
    "                    value=None,\n",
    "                    label=\"Modelo 4 (opcional)\"\n",
    "                )\n",
    "        \n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"### ‚öôÔ∏è Configuraci√≥n\")\n",
    "            \n",
    "            iterations_slider = gr.Slider(\n",
    "                minimum=1, maximum=30, value=5, step=1,\n",
    "                label=\"üîÑ Iteraciones\",\n",
    "                info=\"M√°s iteraciones = m√°s fiable pero m√°s lento\"\n",
    "            )\n",
    "            \n",
    "            temperature = gr.Slider(\n",
    "                minimum=0.0, maximum=1.0, value=0.7, step=0.1,\n",
    "                label=\"üå°Ô∏è Temperature\"\n",
    "            )\n",
    "            \n",
    "            top_p = gr.Slider(\n",
    "                minimum=0.0, maximum=1.0, value=0.9, step=0.05,\n",
    "                label=\"üé≤ Top P\"\n",
    "            )\n",
    "            \n",
    "            max_tokens = gr.Slider(\n",
    "                minimum=100, maximum=2000, value=500, step=100,\n",
    "                label=\"üìè Max Tokens\"\n",
    "            )\n",
    "            \n",
    "            reference_file = gr.File(\n",
    "                label=\"üìÑ Respuesta de referencia (opcional)\",\n",
    "                file_types=[\".txt\"],\n",
    "                type=\"binary\"\n",
    "            )\n",
    "            \n",
    "            # Estimaci√≥n de coste\n",
    "            cost_estimate = gr.Markdown(\"*Selecciona modelos para ver estimaci√≥n*\")\n",
    "    \n",
    "    # Actualizar estimaci√≥n cuando cambian modelos/iteraciones\n",
    "    for component in [model1, model2, model3, model4, iterations_slider]:\n",
    "        component.change(\n",
    "            fn=get_cost_estimate,\n",
    "            inputs=[model1, model2, model3, model4, iterations_slider],\n",
    "            outputs=[cost_estimate]\n",
    "        )\n",
    "    \n",
    "    # === BOT√ìN ===\n",
    "    run_btn = gr.Button(\n",
    "        \"üöÄ Ejecutar Benchmark\",\n",
    "        variant=\"primary\",\n",
    "        size=\"lg\"\n",
    "    )\n",
    "    \n",
    "    status_msg = gr.Markdown(\n",
    "        \"üí° **Configura y ejecuta el benchmark para ver resultados**\"\n",
    "    )\n",
    "    \n",
    "    # === OUTPUTS ===\n",
    "    gr.Markdown(\"---\")\n",
    "    gr.Markdown(\"## üìä Resultados\")\n",
    "    \n",
    "    with gr.Tabs():\n",
    "        with gr.Tab(\"üìã Datos Raw\"):\n",
    "            raw_results = gr.Dataframe(\n",
    "                label=\"Todas las mediciones\",\n",
    "                wrap=True\n",
    "            )\n",
    "            export_btn = gr.Button(\"üíæ Exportar a CSV\")\n",
    "            export_file = gr.File(label=\"Descargar CSV\")\n",
    "        \n",
    "        with gr.Tab(\"üìà Estad√≠sticas Agregadas\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            **Estad√≠sticas por modelo:**\n",
    "            - `mean`: Promedio\n",
    "            - `std`: Desviaci√≥n est√°ndar (consistencia)\n",
    "            - `min/max`: Valores extremos\n",
    "            \"\"\")\n",
    "            agg_results = gr.Dataframe(\n",
    "                label=\"Agregado por modelo\",\n",
    "                wrap=True\n",
    "            )\n",
    "        \n",
    "        with gr.Tab(\"‚ö†Ô∏è Error Rate\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            **M√©tricas de calidad operativa:**\n",
    "            - `error_rate_%`: Porcentaje de llamadas fallidas\n",
    "            - `availability_%`: Porcentaje de llamadas exitosas\n",
    "            - `total_retries`: Reintentos necesarios\n",
    "            \"\"\")\n",
    "            error_results = gr.Dataframe(\n",
    "                label=\"Estad√≠sticas de errores\",\n",
    "                wrap=True\n",
    "            )\n",
    "        \n",
    "        with gr.Tab(\"üìä Coste vs Latencia\"):\n",
    "            comparison_chart = gr.Plot(\n",
    "                label=\"Coste vs Velocidad (Tama√±o = tokens/sec, Color = legibilidad)\"\n",
    "            )\n",
    "        \n",
    "        with gr.Tab(\"‚è±Ô∏è TTFT\"):\n",
    "            ttft_chart = gr.Plot(\n",
    "                label=\"Time to First Token por modelo\"\n",
    "            )\n",
    "    \n",
    "    # === STATE ===\n",
    "    results_state = gr.State()\n",
    "    \n",
    "    # === CONNECTIONS ===\n",
    "    run_btn.click(\n",
    "        fn=run_benchmark_ui,\n",
    "        inputs=[\n",
    "            question_input,\n",
    "            model1, model2, model3, model4,\n",
    "            iterations_slider,\n",
    "            temperature,\n",
    "            top_p,\n",
    "            max_tokens,\n",
    "            reference_file\n",
    "        ],\n",
    "        outputs=[\n",
    "            raw_results,\n",
    "            agg_results,\n",
    "            error_results,\n",
    "            comparison_chart,\n",
    "            ttft_chart,\n",
    "            status_msg\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    raw_results.change(\n",
    "        fn=lambda df: df,\n",
    "        inputs=[raw_results],\n",
    "        outputs=[results_state]\n",
    "    )\n",
    "    \n",
    "    export_btn.click(\n",
    "        fn=export_to_csv,\n",
    "        inputs=[results_state],\n",
    "        outputs=[export_file]\n",
    "    )\n",
    "    \n",
    "    # === DOCUMENTACI√ìN ===\n",
    "    gr.Markdown(\"\"\"\n",
    "    ---\n",
    "    ### üìñ Gu√≠a de M√©tricas\n",
    "    \n",
    "    | M√©trica | Qu√© mide | Bueno es... |\n",
    "    |---------|----------|-------------|\n",
    "    | `latency_sec` | Tiempo total de respuesta | Bajo |\n",
    "    | `ttft_sec` | Time to First Token (UX) | Bajo (<1s ideal) |\n",
    "    | `tokens_per_sec` | Velocidad de generaci√≥n | Alto |\n",
    "    | `cost_usd` | Coste en d√≥lares | Bajo |\n",
    "    | `lexical_diversity` | Variedad de vocabulario | 0.6-0.8 |\n",
    "    | `readability_flesch` | Facilidad de lectura | 60-70 |\n",
    "    | `error_rate_%` | Porcentaje de fallos | Bajo (<5%) |\n",
    "    | `f1_score` | Similitud con referencia | Alto |\n",
    "    \n",
    "    ### üí° Tips\n",
    "    - Usa **5-10 iteraciones** para resultados fiables\n",
    "    - Compara `mean` para rendimiento t√≠pico, `std` para consistencia\n",
    "    - **TTFT bajo** = mejor experiencia de usuario\n",
    "    - **Error rate** indica fiabilidad operativa del modelo/provider\n",
    "    \n",
    "    ### üéØ Casos de uso\n",
    "    1. **Selecci√≥n de modelo**: Encuentra el mejor balance coste/calidad\n",
    "    2. **Optimizaci√≥n de prompts**: Mide impacto de cambios\n",
    "    3. **Monitoreo**: Detecta regresiones en modelos\n",
    "    4. **Presupuesto**: Estima costes antes de ejecutar\n",
    "    \"\"\")\n",
    "\n",
    "print(\"‚úÖ Interfaz Gradio creada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìã PASO 12: Lanzar la aplicaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lanzar aplicaci√≥n\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(\n",
    "        server_name=\"0.0.0.0\",\n",
    "        server_port=7868,\n",
    "        share=True  # Genera link p√∫blico autom√°ticamente\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ commonBench v2 est√° corriendo en puerto 7868\")\n",
    "print(\"‚úÖ Link compartible generado autom√°ticamente\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìã AP√âNDICE: M√©tricas y Competencias\n",
    "\n",
    "### M√©tricas implementadas vs. Frameworks profesionales\n",
    "\n",
    "| Categor√≠a | commonBench v2 | HELM (Stanford) | Notas |\n",
    "|-----------|---------------|-----------------|-------|\n",
    "| Latencia | ‚úÖ `latency_sec` | ‚úÖ | Est√°ndar |\n",
    "| TTFT | ‚úÖ `ttft_sec` | ‚ö†Ô∏è Parcial | UX cr√≠tica |\n",
    "| Throughput | ‚úÖ `tokens_per_sec` | ‚úÖ | Est√°ndar |\n",
    "| Coste | ‚úÖ Din√°mico | ‚ùå | Ventaja |\n",
    "| Error rate | ‚úÖ `error_rate_%` | ‚úÖ | Calidad operativa |\n",
    "| Legibilidad | ‚úÖ Flesch | ‚úÖ | Est√°ndar |\n",
    "| Diversidad | ‚úÖ TTR | ‚úÖ | Est√°ndar |\n",
    "\n",
    "### Competencias demostradas\n",
    "\n",
    "**\"Familiarity with evaluation metrics and quality assessment frameworks\":**\n",
    "\n",
    "1. **Operational vs Model Quality**: Distinguir entre calidad de respuestas y fiabilidad del servicio\n",
    "2. **TTFT**: Entender m√©tricas de UX espec√≠ficas de LLMs\n",
    "3. **Error rate como m√©trica**: No ignorar fallos, medirlos\n",
    "4. **Precios din√°micos**: Veracidad de m√©tricas de coste\n",
    "5. **Reproducibilidad**: Metadata completa para auditor√≠a\n",
    "\n",
    "### Vocabulario para entrevistas\n",
    "\n",
    "- \"Operational Quality\" vs \"Model Quality\"\n",
    "- \"Time to First Token (TTFT)\"\n",
    "- \"Error rate como m√©trica de calidad\"\n",
    "- \"Reproducibilidad temporal\"\n",
    "- \"Exponential backoff con jitter\"\n",
    "- \"Cach√© con TTL\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
